"""
CPEN 400Q work

Helper functions to calculate the metrics such as KL-divergence of the models

Author : @abhishekabhishek
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
from pennylane import numpy as pnp
from jax import numpy as jnp

import metrics


def filter_probs(probs, data):
    """
    filter the 2**n_qubits bitstring probability values to use only the ones
    from the bitstrings that show up in our dataset

    Args:
        probs (qml.np.tensor) : Bitstring probabilities output by the qnode
        data (np.ndarray) : (n_samples, n_features) matrix of our training
            dataset

    Returns:
        probs (qml.np.tensor) : Filtered probability vector containing values
            only for bitstrings that show up in the dataset
    """
    assert len(data.shape) == 2, f"the data matrix is not 2-dimensional : \
        {data.shape}"
    idxs = np.dot(data, 2**np.arange(data.shape[1])[::-1])
    return probs[idxs]


def kl_divergence_estimate(probs):
    """
    analytically calculate the kl-divergence using the probabilities assigned
    by the QCBM to the dataset bitstring using Born's rule

    Args:
        probs (qml.np.tensor) : Model probabilities for each of the bitstrings
            in our dataset

    Returns:
        (qml.np.tensor) : Floating point value of an estimate KL-divergence

    # TODO can this be used as a cost function to train the QCBM
    """
    # Uniform empirical probability of the bitstrings in the dataset
    p_data = pnp.tensor(1./len(probs), requires_grad=False)
    log_p_over_q = pnp.log(probs) - pnp.log(p_data)
    return pnp.dot(probs, log_p_over_q)


def kl_divergence(probs, data):
    """
    compute the kl divergence over the entire space of the bitstrings

    Args:
        probs (qml.np.tensor) : Output of the qnode - probabilities for each
            basis state

    Returns:
        (qml.np.tensor) : Floating point value of the KL-divergence
    """
    # p_data with epsilon added to avoid log(0) error
    p_data = pnp.zeros(len(probs)) + 1e-16
    idxs = pnp.dot(data, 2**pnp.arange(data.shape[1])[::-1])
    p_data[idxs] = 1./data.shape[0]
    log_p_over_q = jnp.log(probs) - jnp.log(p_data)
    return jnp.dot(probs, log_p_over_q)


def plot_BAS(data):
    """
    make a 6x5 grid plot showing different possibilties in the BAS dataset

    Args:
        data (np.ndarray): (30, 4, 4) np.int32 array containing the possible
            patterns in the BAS dataset

    Returns:
        fig (plt.figure), ax (axes.Axes)
    """
    assert len(data.shape) == 3, f'data should be 3-dimensional, given data \
        has {len(data.shape)} dimensions'

    assert data.shape == (30, 4, 4), f'data should have dimensions (30, 4, 4) \
        , given data has dimensions {data.shape}'

    cmap = matplotlib.colors.ListedColormap(['white', 'black'])
    fig, axes = plt.subplots(5, 6, figsize=(18, 15))
    for i in range(data.shape[0]):
        row, col = i//6, i%6
        ax = axes[row][col]
        ax.imshow(data[i], cmap=cmap)
        ax.set_xticks([])
        ax.set_yticks([])
    plt.subplots_adjust(wspace=0.05, hspace=0.05)
    return fig, ax


def plot_top_samples(samples):
    """
    plot the top 30 samples generated by the model

    Args:
        samples (qml.np.Tensor) : Samples generated by the model

    Returns:
        fig (plt.figure), ax (axes.Axes)
    """
    unique_samples, counts = np.unique(samples, return_counts=True, axis=0)
    idxs = np.argsort(counts)
    top_samples = np.zeros((30, 4, 4))
    for i in range(1, 31):
        print(f"Frequency : {counts[idxs[-i]]}")
        top_samples[i-1] = unique_samples[idxs[-i]].reshape(4, 4)

    fig, ax = plot_BAS(top_samples)
    return fig, ax
